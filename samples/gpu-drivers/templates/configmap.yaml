apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ include "app.name" . }}-script
  labels:
    app: nsenter
data:
  downloadandinstall: |
    #!/usr/bin/env bash
    set -uo pipefail

    # This script should be executed on VM host in the directly as the deb packages
    # the host will be mounted at /host, the debs will be copied to /mnt
    # then the container will nsenter and install everything against the host.

    set -x

    open_devices="$(lsof /dev/nvidia* 2>/dev/null)"
    nvidia_modprobe_active="$(systemctl is-active nvidia-modprobe)"
    nvidia_device_plugin_active="$(systemctl is-active nvidia-device-plugin)"

    echo "Open devices: $open_devices"
    echo "nvidia-modprobe active: $nvidia_modprobe_active"
    echo "nvidia-device-plugin active: $nvidia_device_plugin_active"

    if [ -n "$open_devices" ]; then
      if [ "$nvidia_modprobe_active" == "active" ]; then
        systemctl stop nvidia-modprobe
      fi
      if [ "$nvidia_device_plugin_active" == "active" ]; then
        systemctl stop nvidia-device-plugin
      fi
    fi


    GPU_DV=510.47.03
    #GPU_DV={{ .Values.gpuDriverVersion }}
    GPU_DEST=/usr/local/nvidia
    log_file_name="/var/log/nvidia-installer-$(date +%s).log"
    KERNEL_NAME=$(uname -r)

    if [ -f "${GPU_DEST}/bin/nvidia-smi" ]; then
      echo "found existing nvidia-smi, checking version..."
      existing_version="$(nvidia-smi | grep "Driver Version" | cut -d' ' -f3)"
      if [ "$existing_version" == "$GPU_DV" ]; then
        echo "desired version $GPU_DV matches existing version $existing_version, exiting early"
        exit 0
      fi
      if [ ! -z "$existing_version" ]; then
        echo "uninstalling driver version $existing_version to install version $GPU_DV"
        # nvidia uninstall requires kubelet to stop, throw a trap here as well as below
        # this covers existing driver case, outside the conditionals cover new drivers
  
        trap 'systemctl restart kubelet' EXIT SIGINT SIGTERM
        systemctl stop kubelet
        ${GPU_DEST}/bin/nvidia-uninstall --silent
      fi
      if [ -z "$existing_version" ]; then
        echo "found nvidia-smi but failed to extract version, continuing with driver install."
        echo "this could lead to errors if previous module was not unloaded"
        echo "reboot/restarting kubelet fixes it."
      fi
    fi

    # !!DANGER!! but necessary because kubelet holds /dev/nvidia* open
    # can't reinstall drivers without forcing that closed
    trap 'systemctl restart kubelet' EXIT SIGINT SIGTERM

    if [ ! -d "${GPU_DEST}" ]; then
      mkdir -p ${GPU_DEST}
    fi

    set -e

    curl -fLS https://us.download.nvidia.com/tesla/$GPU_DV/NVIDIA-Linux-x86_64-${GPU_DV}.run -o ${GPU_DEST}/nvidia-drivers-${GPU_DV}
    echo "running installer"
    systemctl stop kubelet
    sh $GPU_DEST/nvidia-drivers-$GPU_DV -s -k=$KERNEL_NAME --log-file-name=${log_file_name} -a --no-drm --dkms --utility-prefix="${GPU_DEST}" --opengl-prefix="${GPU_DEST}" 2>&1
    nvidia-modprobe -u -c0
    ldconfig
    systemctl start kubelet
    echo "finished installer"